{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "code_ml_notebook.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOq34_usS2_f"
      },
      "source": [
        "# Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYV2e_e3THck"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gztm2ItZTKJ0"
      },
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KOdlV4TTLR1"
      },
      "source": [
        "# clean function\n",
        "import re\n",
        "import nltk.corpus\n",
        "import nltk \n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "def clean_text(df, text_field):\n",
        "    df[text_field] = df[text_field].str.lower()\n",
        "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", str(elem)))  \n",
        "    # remove numbers\n",
        "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"\\d+\", \"\", str(elem)))  \n",
        "    # stop words\n",
        "    stop = stopwords.words('english')\n",
        "    df[text_field] = df[text_field].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "    # tokens\n",
        "    df[text_field] =  df[text_field].apply(lambda x: word_tokenize(x))\n",
        "    # lemmatization\n",
        "    def word_lemmatizer(text):\n",
        "      lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]\n",
        "      return lem_text\n",
        "    df[text_field] = df[text_field].apply(lambda x: word_lemmatizer(x))\n",
        "    df[text_field] = df[text_field].apply(lambda x: ' '.join(x))\n",
        "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\",  str(elem)))  \n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwOBKSTKTN9u"
      },
      "source": [
        "train = clean_text(train, 'TITLE')\n",
        "train = clean_text(train, 'DESCRIPTION')\n",
        "train = clean_text(train, 'BULLET_POINTS')\n",
        "train = clean_text(train, 'BRAND')\n",
        "\n",
        "test = clean_text(test, 'TITLE')\n",
        "test = clean_text(test, 'DESCRIPTION')\n",
        "test = clean_text(test, 'BULLET_POINTS')\n",
        "test = clean_text(test, 'BRAND')\n",
        "\n",
        "train['text'] = train['TITLE'] + \" \" + train['DESCRIPTION'] + \" \" + train['BULLET_POINTS'] + \" \" + train['BRAND']\n",
        "test['text'] = test['TITLE'] + \" \" + test['DESCRIPTION'] + \" \" + test['BULLET_POINTS'] + \" \" + test['BRAND']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgI1pgpBWwwG"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-07-31T17:13:02.591347Z",
          "start_time": "2021-07-31T17:13:02.254995Z"
        },
        "id": "1261b44e"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-07-31T17:14:12.793526Z",
          "start_time": "2021-07-31T17:13:34.315394Z"
        },
        "id": "3a9f1332"
      },
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-07-31T17:14:20.540599Z",
          "start_time": "2021-07-31T17:14:19.779002Z"
        },
        "id": "d21c12a1"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "\n",
        "sgd = Pipeline([('vect', CountVectorizer()),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
        "               ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-07-31T17:15:08.810279Z",
          "start_time": "2021-07-31T17:15:07.496459Z"
        },
        "id": "74e1a012"
      },
      "source": [
        "train.fillna(' ',inplace =True )\n",
        "test.fillna(' ',inplace =True )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cafb86cf"
      },
      "source": [
        "# from tqdm.notebook import tqdm\n",
        "submission = pd.DataFrame()\n",
        "submission['PRODUCT_ID'] = [i for i in range(1:len(test)+1)]\n",
        "submission['BROWSE_NODE_ID'] = y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T03:59:15.044925Z",
          "start_time": "2021-07-31T17:15:21.759206Z"
        },
        "id": "e701972e"
      },
      "source": [
        "sgd.fit(train['TITLE'], train['BROWSE_NODE_ID'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T04:07:01.953890Z",
          "start_time": "2021-08-01T04:00:50.153466Z"
        },
        "id": "298a1209"
      },
      "source": [
        "y_pred = sgd.predict(test['TITLE'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T04:07:43.847866Z",
          "start_time": "2021-08-01T04:07:43.507560Z"
        },
        "id": "0a85fcc2"
      },
      "source": [
        "# from tqdm.notebook import tqdm\n",
        "submission = pd.DataFrame()\n",
        "submission['PRODUCT_ID'] = [i for i in range(1,len(test)+1)]\n",
        "submission['BROWSE_NODE_ID'] = y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebZ9dcQVZQBw"
      },
      "source": [
        "submission.to_csv('tempsvc.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeE3IY5nUW_l"
      },
      "source": [
        "# FastText Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlhjheGlVytw"
      },
      "source": [
        "!git clone https://github.com/facebookresearch/fastText.git\n",
        "%cd fastText\n",
        "!sudo pip install .\n",
        "!sudo python setup.py install\n",
        "%cd ../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF6AnERRV3QE"
      },
      "source": [
        "text = 'text2'\n",
        "label = 'BROWSE_NODE_ID'\n",
        "train['text2'] = train['TITLE'] + \" \" + train['DESCRIPTION']\n",
        "train = train[[text,label]].copy()\n",
        "train[text].fillna('#', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_NcYjztWbnu"
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "f = open('train.txt', 'w')\n",
        "for index, row in tqdm(train.iterrows()):\n",
        "  f.write('__label__'+ str(row[label]) +' '+row[text]+'\\n')\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zba37n0RWcHu"
      },
      "source": [
        "import fasttext\n",
        "model = fasttext.train_supervised(input=\"train.txt\", lr=0.5, epoch=2, \n",
        "                                  wordNgrams=2, dim=50, \n",
        "                                  loss='hs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVs3bmulWePN"
      },
      "source": [
        "test['text2'] = test['TITLE'] + \" \" + test['DESCRIPTION']\n",
        "test = test[[text]].copy()\n",
        "test[text].fillna('#', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK1MVY9WWpEP"
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "submission = pd.DataFrame()\n",
        "submission['PRODUCT_ID'] = test['PRODUCT_ID']\n",
        "submission['BROWSE_NODE_ID'] = 0;\n",
        "for index, row in tqdm(submission.iterrows()):\n",
        "  tempText = test.iloc[index][text]\n",
        "  prediction = model.predict(tempText)\n",
        "  submission.iloc[index]['BROWSE_NODE_ID'] = int(prediction[0][0][9:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugkk7Fh2Wtp4"
      },
      "source": [
        "submission.to_csv('fastTextSubmission.csv', index=False )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bK3_QVqkZQmv"
      },
      "source": [
        "# Bert (Transformers Library)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2R1akagZQxM"
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import TensorDataset\n",
        "from transformers import BertForSequenceClassification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-yLS4D_aBZr"
      },
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "df = train[['TITLE', 'BROWSE_NODE_ID']].copy()\n",
        "del train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwaNIsEVZ9GR"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(df.index.values, \n",
        "                                                  df.label.values, \n",
        "                                                  test_size=0.15, \n",
        "                                                  random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUd3qhpOaOMz"
      },
      "source": [
        "df['data_type'] = ['not_set']*df.shape[0]\n",
        "\n",
        "df.loc[X_train, 'data_type'] = 'train'\n",
        "df.loc[X_val, 'data_type'] = 'val'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg16VMbqaPCZ"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
        "                                          do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjeXE4u8aQRK"
      },
      "source": [
        "encoded_data_train = tokenizer.batch_encode_plus(\n",
        "    df[df.data_type=='train'].Title.values, \n",
        "    add_special_tokens=True,\n",
        "    return_attention_mask=True, \n",
        "    pad_to_max_length=True, \n",
        "    max_length=256, \n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "encoded_data_val = tokenizer.batch_encode_plus(\n",
        "    df[df.data_type=='val'].Title.values, \n",
        "    add_special_tokens=True, \n",
        "    return_attention_mask=True, \n",
        "    pad_to_max_length=True, \n",
        "    max_length=256, \n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "input_ids_train = encoded_data_train['input_ids']\n",
        "attention_masks_train = encoded_data_train['attention_mask']\n",
        "labels_train = torch.tensor(df[df.data_type=='train'].label.values)\n",
        "\n",
        "input_ids_val = encoded_data_val['input_ids']\n",
        "attention_masks_val = encoded_data_val['attention_mask']\n",
        "labels_val = torch.tensor(df[df.data_type=='val'].label.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ9gTLWWaRid"
      },
      "source": [
        "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
        "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHnUX76raTzv"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
        "                                                      num_labels=len(label_dict),\n",
        "                                                      output_attentions=False,\n",
        "                                                      output_hidden_states=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doCB_2GeaUed"
      },
      "source": [
        "# Bert (from Tez Library)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaTbq94xOvYr"
      },
      "source": [
        "%%capture\n",
        "!pip install tez\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQZdE1KyOvYu"
      },
      "source": [
        "import torch.nn as nn\n",
        "import transformers\n",
        "import torch\n",
        "import tez\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCy3PQBUOvYu"
      },
      "source": [
        "# Taken from https://github.com/abhishekkrthakur/tez/blob/main/examples/text_classification/binary.py\n",
        "\n",
        "class BERTDataset:\n",
        "    def __init__(self, text, target):\n",
        "        self.text = text\n",
        "        self.target = target\n",
        "        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n",
        "            \"bert-base-uncased\", do_lower_case=True\n",
        "        )\n",
        "        self.max_len = 64\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = str(self.text[item])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "        )\n",
        "\n",
        "        ids = inputs[\"input_ids\"]\n",
        "        mask = inputs[\"attention_mask\"]\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "        return {\n",
        "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
        "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
        "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            \"targets\": torch.tensor(self.target[item], dtype=torch.long),\n",
        "        }\n",
        "\n",
        "class BERTBaseUncased(tez.Model):\n",
        "    def __init__(self, num_train_steps, num_classes):\n",
        "        super().__init__()\n",
        "        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n",
        "            \"bert-base-uncased\", do_lower_case=True\n",
        "        )\n",
        "        self.bert = transformers.BertModel.from_pretrained(\"bert-base-uncased\",\n",
        "                                                           return_dict=False)\n",
        "        self.bert_drop = nn.Dropout(0.3)\n",
        "        self.out = nn.Linear(768, num_classes)\n",
        "\n",
        "        self.num_train_steps = num_train_steps\n",
        "        self.step_scheduler_after = \"batch\"\n",
        "\n",
        "    def fetch_optimizer(self):\n",
        "        param_optimizer = list(self.named_parameters())\n",
        "        no_decay = [\"bias\", \"LayerNorm.bias\"]\n",
        "        optimizer_parameters = [\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
        "                ],\n",
        "                \"weight_decay\": 0.001,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
        "                ],\n",
        "                \"weight_decay\": 0.0,\n",
        "            },\n",
        "        ]\n",
        "        opt = AdamW(optimizer_parameters, lr=3e-5)\n",
        "        return opt\n",
        "\n",
        "    def fetch_scheduler(self):\n",
        "        sch = get_linear_schedule_with_warmup(\n",
        "            self.optimizer, num_warmup_steps=0, num_training_steps=self.num_train_steps\n",
        "        )\n",
        "        return sch\n",
        "\n",
        "    def loss(self, outputs, targets):\n",
        "        if targets is None:\n",
        "            return None\n",
        "        return nn.CrossEntropyLoss()(outputs, targets)\n",
        "\n",
        "    def monitor_metrics(self, outputs, targets):\n",
        "        if targets is None:\n",
        "            return {}\n",
        "        outputs = torch.argmax(outputs, dim=1).cpu().detach().numpy()\n",
        "        targets = targets.cpu().detach().numpy()\n",
        "        accuracy = metrics.accuracy_score(targets, outputs)\n",
        "        return {\"accuracy\": accuracy}\n",
        "\n",
        "    def forward(self, ids, mask, token_type_ids, targets=None):\n",
        "        _, o_2 = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
        "        b_o = self.bert_drop(o_2)\n",
        "        output = self.out(b_o)\n",
        "        loss = self.loss(output, targets)\n",
        "        acc = self.monitor_metrics(output, targets)\n",
        "        return output, loss, acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1xWT12_OvYw"
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIjS4jwLc0OZ"
      },
      "source": [
        "train = pd.read_csv('train.csv')[['TITLE','BROWSE_NODE_ID']]\n",
        "train.fillna('', inplace=True)\n",
        "test = pd.read_csv('test.csv')\n",
        "test.fillna('', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgEEIvqEOvYx"
      },
      "source": [
        "encoder = LabelEncoder()\n",
        "train.BROWSE_NODE_ID = encoder.fit_transform(train.BROWSE_NODE_ID)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50DKAq9dOvYy"
      },
      "source": [
        "train, val = train_test_split(train, test_size=0.1, random_state=23)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBSmLrcVOvYz"
      },
      "source": [
        "train_dataset = BERTDataset(\n",
        "    train.TITLE.values,\n",
        "    train.BROWSE_NODE_ID.values\n",
        ")\n",
        "valid_dataset = BERTDataset(\n",
        "    val.TITLE.values,\n",
        "    val.BROWSE_NODE_ID.values\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwK7npKBOvYz"
      },
      "source": [
        "batch = 32\n",
        "n_train_steps = int(len(train) / batch * 2)\n",
        "model = BERTBaseUncased(num_train_steps=n_train_steps, num_classes=len(encoder.classes_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkRz8gLQOvY1"
      },
      "source": [
        "%%capture\n",
        "model.fit(\n",
        "        train_dataset,\n",
        "        valid_dataset=valid_dataset,\n",
        "        train_bs=batch,\n",
        "        device=\"cuda\",\n",
        "        epochs=2,\n",
        "        n_jobs = 2,\n",
        "        fp16=True,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5oamPruyd19"
      },
      "source": [
        "### Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "booUxVWhOvY3"
      },
      "source": [
        "%%capture\n",
        "test_dataset = BERTDataset(\n",
        "        test['TITLE'].values,\n",
        "        [0]*len(test)\n",
        "    )\n",
        "predictionGenerator = model.predict(test_dataset, batch_size=batch, n_jobs=-1)\n",
        "predictions = []\n",
        "for probs in predictionGenerator:\n",
        "    predictions.extend(np.argmax(probs, axis=1))\n",
        "predictions = encoder.inverse_transform(predictions)\n",
        "submission = pd.DataFrame()\n",
        "submission['PRODUCT_ID'] = [i for i in range(1,len(test)+1)]\n",
        "submission['BROWSE_NODE_ID'] = predictions\n",
        "save_file.to_csv('bert.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}